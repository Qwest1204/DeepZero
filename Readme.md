# ğŸ® DeepZero

**DeepZero** â€” Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AlphaZero Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ³Ñ€. ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ (self-play), Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)

## ğŸ¯ ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹

| Ğ˜Ğ³Ñ€Ğ° | Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ´Ğ¾ÑĞºĞ¸ | Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ | ĞšĞ°Ğ½Ğ°Ğ»Ñ‹ | Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ |
|------|--------------|----------|--------|-----------|
| âŒâ­• ĞšÑ€ĞµÑÑ‚Ğ¸ĞºĞ¸-Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¸ | 3Ã—3 | 9 | 3 | â­ |
| ğŸ”´ğŸŸ¡ Ğ§ĞµÑ‚Ñ‹Ñ€Ğµ Ğ² Ñ€ÑĞ´ | 6Ã—7 | 7 | 3 | â­â­ |
| âš«âšª Ğ¨Ğ°ÑˆĞºĞ¸ | 8Ã—8 | 4096 | 5 | â­â­â­ |
| â™Ÿï¸â™š Ğ¨Ğ°Ñ…Ğ¼Ğ°Ñ‚Ñ‹ | 8Ã—8 | 4096 | 13 | â­â­â­â­ |

## ğŸ§  ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼

DeepZero Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ **Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸** Ğ¸ **Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS)**.

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DeepZero                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   Ğ˜Ğ³Ñ€Ğ¾Ğ²Ğ°Ñ   â”‚ â”€â”€â”€â–¶ â”‚   ResNet    â”‚ â”€â”€â”€â–¶ â”‚   MCTS     â”‚  â”‚
â”‚   â”‚    ÑÑ€ĞµĞ´Ğ°    â”‚      â”‚  (policy,   â”‚      â”‚  (Ğ¿Ğ¾Ğ¸ÑĞº)   â”‚  â”‚
â”‚   â”‚             â”‚ â—€â”€â”€â”€ â”‚   value)    â”‚ â—€â”€â”€â”€ â”‚            â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹

#### 1. ğŸ² Ğ˜Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ (`games/`)
ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ³Ñ€Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ:

```python
class Game:
    def get_initial_state(self)           # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ
    def get_next_state(state, action, player)  # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ…Ğ¾Ğ´
    def get_valid_moves(state)            # ĞœĞ°ÑĞºĞ° Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ñ… Ñ…Ğ¾Ğ´Ğ¾Ğ²
    def check_win(state, action)          # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹
    def get_value_and_terminated(state, action)  # Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ
    def change_perspective(state, player) # Ğ¡Ğ¼ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹
    def get_encoded_state(state)          # ĞšĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸
```

#### 2. ğŸ§¬ ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ ResNet (`models/resnet.py`)
ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸:

```
Input: encoded_state [channels Ã— height Ã— width]
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Conv Block  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ResBlocks  â”‚ Ã— N
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Policy â”‚ â”‚ Value  â”‚
â”‚  Head  â”‚ â”‚  Head  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â–¼           â–¼
 Ï€(s,a)       v(s)
```

- **Policy Head** `Ï€(s,a)`: Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
- **Value Head** `v(s)`: ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ [-1, 1]

#### 3. ğŸŒ³ MCTS (`models/mcts.py`)
ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸:

```
          Selection          Expansion         Simulation        Backpropagation
              â”‚                  â”‚                  â”‚                   â”‚
              â–¼                  â–¼                  â–¼                   â–¼
           â”Œâ”€â”€â”€â”              â”Œâ”€â”€â”€â”              â”Œâ”€â”€â”€â”              â”Œâ”€â”€â”€â”
           â”‚ â— â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â— â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â— â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â— â”‚
           â””â”€â”¬â”€â”˜              â””â”€â”¬â”€â”˜              â””â”€â”¬â”€â”˜              â””â”€â”¬â”€â”˜
           â”Œâ”€â”´â”€â”              â”Œâ”€â”´â”€â”              â”Œâ”€â”´â”€â”              â”Œâ”€â”´â”€â”
           â”‚   â”‚              â”‚   â”‚              â”‚   â”‚              â”‚   â”‚
          â—   â—              â—   â—              â—   â—â”€â”€â–¶NN        â—   â—
                                  â”‚                  â”‚                 â–²
                                  â–¼                  â–¼                 â”‚
                                  â—‹              v=0.7 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**UCB Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑƒĞ·Ğ»Ğ°:**
```
UCB(s,a) = Q(s,a) + C Ã— Ï€(s,a) Ã— âˆš(N(s)) / (1 + N(s,a))
```

#### 4. ğŸ”„ Self-Play (`models/deepzero.py`)
Ğ¦Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  1. Self-Play (Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  for game in parallel_games:             â”‚        â”‚
â”‚     â”‚      state = initial_state               â”‚        â”‚
â”‚     â”‚      while not terminated:               â”‚        â”‚
â”‚     â”‚          Ï€ = MCTS.search(state)          â”‚        â”‚
â”‚     â”‚          action = sample(Ï€)              â”‚        â”‚
â”‚     â”‚          memory.append(state, Ï€)         â”‚        â”‚
â”‚     â”‚          state = next_state(action)      â”‚        â”‚
â”‚     â”‚      assign_values(memory, winner)       â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚  2. Training (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸)                       â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  for epoch in epochs:                    â”‚        â”‚
â”‚     â”‚      for batch in memory:                â”‚        â”‚
â”‚     â”‚          Ï€_pred, v_pred = model(states)  â”‚        â”‚
â”‚     â”‚          loss = CE(Ï€_pred, Ï€_target)     â”‚        â”‚
â”‚     â”‚                + MSE(v_pred, v_target)   â”‚        â”‚
â”‚     â”‚          optimizer.step()                â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                          â”‚                               â”‚
â”‚                          â–¼                               â”‚
â”‚  3. Save checkpoint                                      â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

```
DeepZero/
â”œâ”€â”€ games/
â”‚   â”œâ”€â”€ tictactoe.py      # ĞšÑ€ĞµÑÑ‚Ğ¸ĞºĞ¸-Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¸
â”‚   â”œâ”€â”€ connectfour.py    # Ğ§ĞµÑ‚Ñ‹Ñ€Ğµ Ğ² Ñ€ÑĞ´
â”‚   â”œâ”€â”€ checkers.py       # Ğ¨Ğ°ÑˆĞºĞ¸
â”‚   â””â”€â”€ chess.py          # Ğ¨Ğ°Ñ…Ğ¼Ğ°Ñ‚Ñ‹
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resnet.py         # ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ
â”‚   â”œâ”€â”€ mcts.py           # ĞŸĞ¾Ğ¸ÑĞº ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾
â”‚   â””â”€â”€ deepzero.py       # Self-play Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
â”œâ”€â”€ train_tictactoe.py    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµÑÑ‚Ğ¸ĞºĞ¾Ğ²-Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ²
â”œâ”€â”€ train_checkers.py     # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ°ÑˆĞµĞº
â”œâ”€â”€ train_chess.py        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ñ…Ğ¼Ğ°Ñ‚
â”œâ”€â”€ play_vs_ai.py         # Ğ˜Ğ³Ñ€Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ˜Ğ˜
â””â”€â”€ README.md
```

## ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

### Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

```txt
numpy>=1.21.0
torch>=2.0.0
tqdm>=4.60.0
```

## ğŸ“ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ

### ĞšÑ€ĞµÑÑ‚Ğ¸ĞºĞ¸-Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¸ (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ~5 Ğ¼Ğ¸Ğ½ÑƒÑ‚)

```python
from games.tictactoe import TicTacToe
from models.resnet import ResNet
from models.deepzero import DeepZeroParallel
import torch

game = TicTacToe()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ResNet(game, num_resBlocks=4, num_hidden=64, device=device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

args = {
    'C': 2,                        # UCB ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ°
    'num_searches': 60,            # MCTS ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹
    'num_iterations': 3,           # Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    'num_parallel_games': 100,     # ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ³Ñ€
    'num_selfPlay_iterations': 500,# Self-play Ğ¸Ğ³Ñ€ Ğ·Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ
    'num_epochs': 4,               # Ğ­Ğ¿Ğ¾Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    'batch_size': 64,
    'temperature': 1.25,           # Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ
    'dirichlet_epsilon': 0.25,     # Ğ¨ÑƒĞ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    'dirichlet_alpha': 0.3
}

deepzero = DeepZeroParallel(model, optimizer, game, args)
deepzero.learn()
```

### Ğ¨Ğ°ÑˆĞºĞ¸ (ÑÑ€ĞµĞ´Ğ½Ğµ, ~2-4 Ñ‡Ğ°ÑĞ° Ğ½Ğ° GPU)

```python
from games.checkers import Checkers
from models.resnet import ResNet
from models.deepzero import DeepZeroParallel
import torch

game = Checkers()
device = torch.device("cuda")

model = ResNet(game, num_resBlocks=9, num_hidden=128, device=device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

args = {
    'C': 2,
    'num_searches': 100,
    'num_iterations': 8,
    'num_parallel_games': 32,
    'num_selfPlay_iterations': 100,
    'num_epochs': 4,
    'batch_size': 64,
    'temperature': 1.25,
    'dirichlet_epsilon': 0.25,
    'dirichlet_alpha': 0.5
}

deepzero = DeepZeroParallel(model, optimizer, game, args)
deepzero.learn()
```

### Ğ¨Ğ°Ñ…Ğ¼Ğ°Ñ‚Ñ‹ (Ğ´Ğ¾Ğ»Ğ³Ğ¾, ~24-48 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° GPU)

```python
from games.chess import Chess
from models.resnet import ResNet
from models.deepzero import DeepZeroParallel
import torch

game = Chess()
device = torch.device("cuda")

model = ResNet(game, num_resBlocks=19, num_hidden=256, device=device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

args = {
    'C': 2,
    'num_searches': 400,
    'num_iterations': 20,
    'num_parallel_games': 64,
    'num_selfPlay_iterations': 200,
    'num_epochs': 4,
    'batch_size': 128,
    'temperature': 1.25,
    'dirichlet_epsilon': 0.25,
    'dirichlet_alpha': 0.3
}

deepzero = DeepZeroParallel(model, optimizer, game, args)
deepzero.learn()
```

## ğŸ® Ğ˜Ğ³Ñ€Ğ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ˜Ğ˜

```python
from games.checkers import Checkers
from models.resnet import ResNet
from models.mcts import MCTS
import torch
import numpy as np

game = Checkers()
device = torch.device("cpu")

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
model = ResNet(game, 9, 128, device=device)
model.load_state_dict(torch.load("model_Checkers_7.pt", map_location=device))
model.eval()

args = {'C': 2, 'num_searches': 600, 'dirichlet_epsilon': 0, 'dirichlet_alpha': 0.3}
mcts = MCTS(game, args, model)

state = game.get_initial_state()
player = 1  # Ğ’Ñ‹ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚Ğµ Ğ±ĞµĞ»Ñ‹Ğ¼Ğ¸

while True:
    game.print_board(state)
    
    if player == 1:
        # Ğ¥Ğ¾Ğ´ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°
        valid_moves = game.get_valid_moves(state)
        valid_actions = np.where(valid_moves == 1)[0]
        
        print("Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ…Ğ¾Ğ´Ñ‹:")
        for i, action in enumerate(valid_actions):
            fr, fc, tr, tc = game.action_to_coords(action)
            print(f"  {i}: ({fr},{fc}) -> ({tr},{tc})")
        
        choice = int(input("Ğ’Ğ°Ñˆ Ñ…Ğ¾Ğ´: "))
        action = valid_actions[choice]
    else:
        # Ğ¥Ğ¾Ğ´ Ğ˜Ğ˜
        neutral_state = game.change_perspective(state, player)
        mcts_probs = mcts.search(neutral_state)
        action = np.argmax(mcts_probs)
        action = game.flip_action(action)
        print(f"Ğ˜Ğ˜ Ñ…Ğ¾Ğ´Ğ¸Ñ‚: {game.action_to_coords(action)}")
    
    state = game.get_next_state(state, action, player)
    value, terminated = game.get_value_and_terminated(state, action)
    
    if terminated:
        game.print_board(state)
        print("Ğ‘ĞµĞ»Ñ‹Ğµ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸!" if value == 1 and player == 1 else "Ğ§Ñ‘Ñ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸!")
        break
    
    player = game.get_opponent(player)
```

## ğŸ“Š ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹

| ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ | TicTacToe | Checkers | Chess |
|----------|----------|-----------|----------|-------|
| `num_resBlocks` | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ residual Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² | 4 | 9 | 19 |
| `num_hidden` | Ğ Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ | 64 | 128 | 256 |
| `num_searches` | MCTS ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ·Ğ° Ñ…Ğ¾Ğ´ | 60 | 100 | 400 |
| `num_iterations` | Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ | 3 | 8 | 20 |
| `num_parallel_games` | ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… self-play Ğ¸Ğ³Ñ€ | 100 | 32 | 64 |
| `dirichlet_alpha` | ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ ÑˆÑƒĞ¼Ğ° Ğ”Ğ¸Ñ€Ğ¸Ñ…Ğ»Ğµ | 0.3 | 0.5 | 0.3 |

## ğŸ“ˆ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ‹:
- `model_{Game}_{iteration}.pt` â€” Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
- `optimizer_{Game}_{iteration}.pt` â€” ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°

## ğŸ”§ API Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´

Ğ’ÑĞµ Ğ¸Ğ³Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ:

```python
class Game:
    row_count: int          # Ğ’Ñ‹ÑĞ¾Ñ‚Ğ° Ğ´Ğ¾ÑĞºĞ¸
    column_count: int       # Ğ¨Ğ¸Ñ€Ğ¸Ğ½Ğ° Ğ´Ğ¾ÑĞºĞ¸
    action_size: int        # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
    shape_obs: int          # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸
    
    def __repr__(self) -> str
    def get_initial_state(self) -> np.ndarray
    def get_next_state(self, state, action, player) -> np.ndarray
    def get_valid_moves(self, state) -> np.ndarray
    def check_win(self, state, action) -> bool
    def get_value_and_terminated(self, state, action) -> Tuple[int, bool]
    def get_opponent(self, player) -> int
    def get_opponent_value(self, value) -> int
    def change_perspective(self, state, player) -> np.ndarray
    def get_encoded_state(self, state) -> np.ndarray
    def flip_action(self, action) -> int
```

## ğŸ“š Ğ›Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°

- [Mastering the Game of Go without Human Knowledge](https://www.nature.com/articles/nature24270) â€” AlphaGo Zero
- [A general reinforcement learning algorithm that masters chess, shogi, and Go](https://www.science.org/doi/10.1126/science.aar6404) â€” AlphaZero
- [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265) â€” MuZero

## ğŸ“ Ğ›Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ

MIT License

## ğŸ¤ Ğ’ĞºĞ»Ğ°Ğ´

Pull requests Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ! Ğ”Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚ĞºÑ€Ğ¾Ğ¹Ñ‚Ğµ issue.

---

<p align="center">
  Made with â¤ï¸ and ğŸ§ 
</p>