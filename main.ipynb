{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-15T14:05:44.761404Z",
     "start_time": "2025-12-15T14:05:43.830631Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from games.tictactoe import TicTacToe\n",
    "from games.connectfour import ConnectFour\n",
    "from models.mcts import MCTS, MCTSParallel\n",
    "from models.resnet import ResNet\n",
    "from models.deepzero import DeepZero, DeepZeroParallel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# tictactoe game",
   "id": "966d13a051eb6121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:59:17.846042Z",
     "start_time": "2025-12-15T13:59:17.844486Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a7b794537a1732c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:05:50.351673Z",
     "start_time": "2025-12-15T14:05:45.434823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ResNet(game, 4, 32, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_search': 100,\n",
    "    'num_iterations': 3,\n",
    "    'num_parallel_games': 100,\n",
    "    'batch_size': 16,\n",
    "    'num_selfplay_iterations': 350,\n",
    "    'num_epochs': 4,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 1\n",
    "}\n",
    "\n",
    "deepzero = DeepZeroParallel(model, optimizer, game, args)\n",
    "deepzero.learn()\n"
   ],
   "id": "83e6c74a1acb2f75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     10\u001B[39m args = {\n\u001B[32m     11\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mC\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m2\u001B[39m,\n\u001B[32m     12\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mnum_search\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m100\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     20\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mdirichlet_alpha\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m1\u001B[39m\n\u001B[32m     21\u001B[39m }\n\u001B[32m     23\u001B[39m deepzero = DeepZeroParallel(model, optimizer, game, args)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mdeepzero\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/deepzero.py:177\u001B[39m, in \u001B[36mDeepZeroParallel.learn\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m \u001B[38;5;28mself\u001B[39m.model.eval()\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m selfPlay_iteration \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_selfplay_iterations\u001B[39m\u001B[33m'\u001B[39m] // \u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_parallel_games\u001B[39m\u001B[33m'\u001B[39m]):\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     memory += \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mselfPlay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[38;5;28mself\u001B[39m.model.train()\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m'\u001B[39m]):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/deepzero.py:113\u001B[39m, in \u001B[36mDeepZeroParallel.selfPlay\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    110\u001B[39m states = np.stack([spg.state \u001B[38;5;28;01mfor\u001B[39;00m spg \u001B[38;5;129;01min\u001B[39;00m spGames])\n\u001B[32m    111\u001B[39m neutral_states = \u001B[38;5;28mself\u001B[39m.game.change_perspective(states, player)\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmcts\u001B[49m\u001B[43m.\u001B[49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mneutral_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspGames\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(spGames))[::-\u001B[32m1\u001B[39m]:\n\u001B[32m    116\u001B[39m     spg = spGames[i]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/.venv/lib/python3.14/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/mcts.py:148\u001B[39m, in \u001B[36mMCTSParallel.search\u001B[39m\u001B[34m(self, states, spGames)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m node.is_fully_expanded():\n\u001B[32m    146\u001B[39m     node = node.select()\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m value, is_terminal = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_value_and_terminated\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43maction_taken\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    149\u001B[39m value = \u001B[38;5;28mself\u001B[39m.game.get_opponent_value(value)\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_terminal:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/games/tictactoe.py:41\u001B[39m, in \u001B[36mTicTacToe.get_value_and_terminated\u001B[39m\u001B[34m(self, state, action)\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_value_and_terminated\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcheck_win\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     42\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m1\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m np.sum(\u001B[38;5;28mself\u001B[39m.get_valid_moves(state)) == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/games/tictactoe.py:36\u001B[39m, in \u001B[36mTicTacToe.check_win\u001B[39m\u001B[34m(self, state, action)\u001B[39m\n\u001B[32m     30\u001B[39m column = action % \u001B[38;5;28mself\u001B[39m.column_count\n\u001B[32m     31\u001B[39m player = state[row, column]\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m     34\u001B[39m         np.sum(state[row, :]) == player * \u001B[38;5;28mself\u001B[39m.column_count\n\u001B[32m     35\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m np.sum(state[:, column]) == player * \u001B[38;5;28mself\u001B[39m.row_count\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m np.sum(\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdiag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m) == player * \u001B[38;5;28mself\u001B[39m.row_count\n\u001B[32m     37\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m np.sum(np.diag(np.flip(state, axis=\u001B[32m0\u001B[39m))) == player * \u001B[38;5;28mself\u001B[39m.row_count\n\u001B[32m     38\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/.venv/lib/python3.14/site-packages/numpy/lib/_twodim_base_impl.py:254\u001B[39m, in \u001B[36m_diag_dispatcher\u001B[39m\u001B[34m(v, k)\u001B[39m\n\u001B[32m    248\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m m\n\u001B[32m    251\u001B[39m _eye_with_like = array_function_dispatch()(eye)\n\u001B[32m--> \u001B[39m\u001B[32m254\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_diag_dispatcher\u001B[39m(v, k=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    255\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (v,)\n\u001B[32m    258\u001B[39m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_diag_dispatcher)\n\u001B[32m    259\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdiag\u001B[39m(v, k=\u001B[32m0\u001B[39m):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:52:08.204746Z",
     "start_time": "2025-12-15T13:51:34.838489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game = TicTacToe()\n",
    "player = -1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_search': 100,\n",
    "    'num_iterations': 3,\n",
    "    'num_parallel_games': 100,\n",
    "    'batch_size': 16,\n",
    "    'num_selfplay_iterations': 350,\n",
    "    'num_epochs': 4,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "\n",
    "model = ResNet(game, 4, 32, device=device)\n",
    "model.load_state_dict(torch.load(\"weights/model_2_TicTacToe.pt\", map_location=device))\n",
    "model.eval()\n",
    "mcts = MCTS(game, args, model)\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"val_movies\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}: \"))\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not val\")\n",
    "            continue\n",
    "    else:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs, net_win_value = mcts.search(neutral_state)\n",
    "        print(\"expected win rate\", net_win_value)\n",
    "        mcts_probs = mcts_probs * valid_moves  # Mask invalid moves to zero\n",
    "        action = np.argmax(mcts_probs)\n",
    "        # Optional: Add a check for no valid moves, though this should not occur in a proper game state\n",
    "        if valid_moves[action] == 0:\n",
    "            raise ValueError(\"No valid moves available; game state may be invalid.\")\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "    value, is_terminate = game.get_value_and_terminated(state, action)\n",
    "    if is_terminate:\n",
    "        if value == 1:\n",
    "            print(player, \"win\")\n",
    "        else:\n",
    "            print(player, \"lose\")\n",
    "        break\n",
    "    player = game.get_opponent(player)"
   ],
   "id": "2e7f7b9aa1788d1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "expected win rate -0.3342079520225525\n",
      "[[ 0. -1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "val_movies [0, 2, 3, 4, 5, 6, 7, 8]\n",
      "[[ 0. -1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "expected win rate 0.16230599582195282\n",
      "[[ 0. -1. -1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "val_movies [0, 3, 4, 5, 6, 7]\n",
      "[[ 1. -1. -1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "expected win rate 0.49623584747314453\n",
      "[[ 1. -1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "val_movies [3, 5, 6, 7]\n",
      "[[ 1. -1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  1.  1.]]\n",
      "expected win rate 0.4345284402370453\n",
      "-1 win\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T13:54:11.613045Z",
     "start_time": "2025-12-15T13:52:59.303743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 64, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_search': 600,\n",
    "    'num_iterations': 8,\n",
    "    'batch_size': 64,\n",
    "    'num_selfplay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "deepzero = DeepZeroParallel(model, optimizer, game, args)\n",
    "deepzero.learn()\n"
   ],
   "id": "269bc8b9c80750d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [01:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     10\u001B[39m args = {\n\u001B[32m     11\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mC\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m2\u001B[39m,\n\u001B[32m     12\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mnum_search\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m600\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     20\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mdirichlet_alpha\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m0.3\u001B[39m\n\u001B[32m     21\u001B[39m }\n\u001B[32m     23\u001B[39m deepzero = DeepZeroParallel(model, optimizer, game, args)\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mdeepzero\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/deepzero.py:177\u001B[39m, in \u001B[36mDeepZeroParallel.learn\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m \u001B[38;5;28mself\u001B[39m.model.eval()\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m selfPlay_iteration \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_selfplay_iterations\u001B[39m\u001B[33m'\u001B[39m] // \u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_parallel_games\u001B[39m\u001B[33m'\u001B[39m]):\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     memory += \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mselfPlay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[38;5;28mself\u001B[39m.model.train()\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;28mself\u001B[39m.args[\u001B[33m'\u001B[39m\u001B[33mnum_epochs\u001B[39m\u001B[33m'\u001B[39m]):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/deepzero.py:113\u001B[39m, in \u001B[36mDeepZeroParallel.selfPlay\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    110\u001B[39m states = np.stack([spg.state \u001B[38;5;28;01mfor\u001B[39;00m spg \u001B[38;5;129;01min\u001B[39;00m spGames])\n\u001B[32m    111\u001B[39m neutral_states = \u001B[38;5;28mself\u001B[39m.game.change_perspective(states, player)\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmcts\u001B[49m\u001B[43m.\u001B[49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mneutral_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspGames\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(spGames))[::-\u001B[32m1\u001B[39m]:\n\u001B[32m    116\u001B[39m     spg = spGames[i]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/.venv/lib/python3.14/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/mcts.py:177\u001B[39m, in \u001B[36mMCTSParallel.search\u001B[39m\u001B[34m(self, states, spGames)\u001B[39m\n\u001B[32m    174\u001B[39m spg_policy *= valid_moves\n\u001B[32m    175\u001B[39m spg_policy /= np.sum(spg_policy)\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspg_policy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m node.backpropagate(spg_value)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/models/mcts.py:46\u001B[39m, in \u001B[36mNode.expand\u001B[39m\u001B[34m(self, policy)\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prob > \u001B[32m0\u001B[39m:\n\u001B[32m     45\u001B[39m     child_state = \u001B[38;5;28mself\u001B[39m.state.copy()\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     child_state = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_next_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchild_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m     child_state = \u001B[38;5;28mself\u001B[39m.game.change_perspective(child_state, player=-\u001B[32m1\u001B[39m)\n\u001B[32m     49\u001B[39m     child = Node(\u001B[38;5;28mself\u001B[39m.game, \u001B[38;5;28mself\u001B[39m.args, child_state, \u001B[38;5;28mself\u001B[39m, action, prob)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DeepZero/games/connectfour.py:17\u001B[39m, in \u001B[36mConnectFour.get_next_state\u001B[39m\u001B[34m(self, state, action, player)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_next_state\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action, player):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     row = np.max(\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m     18\u001B[39m     state[row, action] = player\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m state\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "device = torch.device(\"cpu\")\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_search': 600,\n",
    "    'num_iterations': 8,\n",
    "    'batch_size': 64,\n",
    "    'num_selfplay_iterations': 500,\n",
    "    'num_epochs': 4,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "model = ResNet(game, 9, 32, device=device)\n",
    "model.eval()\n",
    "mcts = MCTS(game, args, model)\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"val_movies\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}: \"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not val\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminate = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminate:\n",
    "        if value == 1:\n",
    "            print(player, \"win\")\n",
    "        else:\n",
    "             print(player, \"lose\")\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ],
   "id": "529975a467b87707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    print(state)\n",
    "    if player == 1:\n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not val\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminate = tictactoe.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminate:\n",
    "        if value == 1:\n",
    "            print(player, \"win\")\n",
    "        else:\n",
    "            print(player, \"lose\")\n",
    "        break\n",
    "\n",
    "    player = tictactoe.get_opponent(player)"
   ],
   "id": "fe0111dc8edff67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "game.get_next_state(state, player)",
   "id": "d4ffdf1b33741ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d8fb4cbd491364c8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
